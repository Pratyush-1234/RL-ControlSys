# -*- coding: utf-8 -*-
"""1st_2nd_non_linear_evs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12DfBjX7bVl-7eGAdac-wk72KgQzbjrpw
"""

__credits__ = ["Carlos Luis"]

import numpy as np
import gymnasium as gym
from gymnasium import spaces

class LinearFirstOrderEnv(gym.Env):
    def __init__(self):
        self.A, self.B, self.C, self.D = -2.0, 3.0, 1.0, 0.0
        self.K, self.lam, self.lam2 = 4, 0.8, 0.5
        self.w1, self.w2 = 1.0, 0.001
        self.dt = 0.05

        self.min_action, self.max_action = -1.0, 1.0
        self.min_state, self.max_state = -10.0, 10.0
        self.min_K, self.max_K = 0.0, 1000.0
        self.min_e, self.max_e = self.min_state - self.max_state, self.max_state - self.min_state
        self.min_edot, self.max_edot = self.min_e / self.dt, self.max_e / self.dt

        self.ref_model_min = self.min_edot + self.K * self.min_e
        self.ref_model_max = self.max_edot + self.K * self.max_e
        self.min_obs = np.array([self.min_state, self.min_state, self.ref_model_min, self.ref_model_min, self.min_K])
        self.max_obs = np.array([self.max_state, self.max_state, self.ref_model_max, self.ref_model_max, self.max_K])

        self.targ_state, self.init_state = 0.0, 1.0
        self.test_targ_state, self.test_init_state = 0.0, 1.0
        self.targ_m = 0.0

        self.observation_space = spaces.Box(low=self.min_obs, high=self.max_obs, dtype=np.float32)
        self.action_space = spaces.Box(low=self.min_action, high=self.max_action, shape=(1,), dtype=np.float32)

    def step(self, action):
        action = action[0]
        x = self.state
        dx = self.A * x + self.B * action
        new_x = x + self.dt * dx

        e_new = self.targ_state - new_x
        e_dot = (e_new - self.e) / self.dt
        m = e_dot + self.K * e_new

        reward = -(self.w1 * abs(m - self.targ_m) + self.lam * abs(e_new) ** 2 + self.lam2 * abs(e_dot) ** 2 + self.w2 * action ** 2)

        self.state, self.e, self.e_dot, self.m = new_x, e_new, e_dot, m
        obs = self.get_obs()
        info = {'e': self.e, 'e_dot': self.e_dot, 'ref_model': self.m}
        return obs, reward, False, False, info

    def reset(self, init_state=None, targ_state=None):
        self.state = init_state if init_state is not None else np.random.uniform(self.min_state, self.max_state)
        self.targ_state = targ_state if targ_state is not None else self.targ_state

        self.e = self.targ_state - self.state
        self.e_dot = 0.0
        self.m = self.e_dot + self.K * self.e

        obs = self.get_obs()
        info = {'init_state': self.state, 'targ_state': self.targ_state, 'e': self.e, 'e_dot': self.e_dot, 'K': self.K, 'model': self.m}
        return obs, info

    def get_obs(self):
        return np.array([self.state, self.targ_state, self.m, self.targ_m, self.K])

import numpy as np
import gymnasium as gym
from gymnasium import spaces

class LinearSecondOrderEnv(gym.Env):
    def __init__(self):
        # System dynamics
        self.A = np.array([[0.0, 1.0], [-1.0, -1.0]])
        self.B = np.array([[0.0], [1.0]])
        self.C = np.array([1.0, 0.0])
        self.D = np.array([0.0])

        # Parameters
        self.K = 0.01
        self.lam = 3.0
        self.lam2 = 0.1
        self.w1 = 1.0
        self.w2 = 0.0001
        self.dt = 0.05

        # State and action limits
        self.min_action, self.max_action = -1.0, 1.0
        self.min_state, self.max_state = np.array([-10.0, -10.0]), np.array([10.0, 10.0])
        self.min_K, self.max_K = 0.0, 1000.0

        # Reference model limits
        self.ref_model_min = np.linalg.norm(-self.max_state + self.K * self.min_state)
        self.ref_model_max = np.linalg.norm(self.max_state + self.K * self.max_state)

        # Observation and action spaces
        self.observation_space = spaces.Box(
            low=np.array([*self.min_state, *self.min_state, self.ref_model_min, self.ref_model_min, self.min_K]),
            high=np.array([*self.max_state, *self.max_state, self.ref_model_max, self.ref_model_max, self.max_K]),
            dtype=np.float32
        )
        self.action_space = spaces.Box(low=self.min_action, high=self.max_action, shape=(1,), dtype=np.float32)

        # Initialization
        self.state = np.array([1.0, 0.0])
        self.targ_state = np.array([0.0, 0.0])
        self.targ_m = 0.0

    def step(self, action):
        action = np.clip(action[0], self.min_action, self.max_action)
        x = self.state

        # State update
        dx = self.A @ x + self.B * action
        new_x = x + self.dt * dx.flatten()

        # Error and reference model calculations
        e_new = self.targ_state[0] - new_x[0]
        e_dot_new = (e_new - (self.targ_state[0] - x[0])) / self.dt
        self.m = np.linalg.norm(e_dot_new + self.K * e_new)

        # Reward function
        reward = -(
            self.w1 * abs(self.m - self.targ_m) +
            self.lam * e_new**2 +
            self.lam2 * e_dot_new**2 +
            self.w2 * action**2
        )

        # Update state
        self.state = new_x
        obs = self.get_obs()
        info = {'e': e_new, 'e_dot': e_dot_new, 'ref_model': self.m}

        return obs, reward, False, False, info

    def reset(self, init_state=None, targ_state=None):
        self.state = (
            np.random.uniform(self.min_state, self.max_state)
            if init_state is None else np.clip(init_state, self.min_state, self.max_state)
        )
        self.targ_state = (
            np.random.uniform(self.min_state, self.max_state)
            if targ_state is None else np.clip(targ_state, self.min_state, self.max_state)
        )

        e_new = self.targ_state[0] - self.state[0]
        e_dot_new = -self.state[1] / self.dt
        self.m = np.linalg.norm(e_dot_new + self.K * e_new)

        obs = self.get_obs()
        info = {
            'init_state': self.state,
            'targ_state': self.targ_state,
            'e': e_new,
            'e_dot': e_dot_new,
            'K': self.K,
            'model': self.m
        }
        return obs, info

    def get_obs(self):
        return np.array([
            *self.state, *self.targ_state, self.m, self.targ_m, self.K
        ], dtype=np.float32)

import numpy as np
import gymnasium as gym
from gymnasium.spaces import Box

class MyPendulumEnv(gym.Env):
    metadata = {"render_modes": ["human", "rgb_array"], "render_fps": 50}

    def __init__(self, g=10.0, render_mode=None):
        super().__init__()
        self.max_speed = 8.0
        self.max_torque = 2.0
        self.dt = 0.05
        self.g = g
        self.m = 1.0  # Mass
        self.l = 1.0  # Length
        self.viewer = None

        # Action space: Continuous torque control
        self.action_space = Box(low=-self.max_torque, high=self.max_torque, shape=(1,), dtype=np.float32)

        # Observation space: Cosine and sine of angle, and angular velocity
        high = np.array([1.0, 1.0, self.max_speed], dtype=np.float32)
        self.observation_space = Box(low=-high, high=high, dtype=np.float32)

        self.state = None
        self.render_mode = render_mode

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        high = np.array([np.pi, self.max_speed], dtype=np.float32)
        self.state = np.random.uniform(low=-high, high=high)
        return self._get_obs(), {}

    def step(self, action):
        theta, theta_dot = self.state
        action = np.clip(action, -self.max_torque, self.max_torque)[0]

        # Equations of motion
        torque = action
        new_theta_dot = theta_dot + (-3 * self.g / (2 * self.l) * np.sin(theta) + 3.0 / (self.m * self.l**2) * torque) * self.dt
        new_theta_dot = np.clip(new_theta_dot, -self.max_speed, self.max_speed)
        new_theta = theta + new_theta_dot * self.dt

        self.state = np.array([new_theta, new_theta_dot], dtype=np.float32)

        # Reward function
        reward = - (new_theta**2 + 0.1 * new_theta_dot**2 + 0.001 * torque**2)

        return self._get_obs(), reward, False, False, {}

    def _get_obs(self):
        theta, theta_dot = self.state
        return np.array([np.cos(theta), np.sin(theta), theta_dot], dtype=np.float32)

    def render(self):
        if self.render_mode == "human":
            return self._render_human()
        elif self.render_mode == "rgb_array":
            return self._render_rgb_array()

    def _render_human(self):
        import pygame
        from pygame import gfxdraw

        screen_width = 500
        screen_height = 500

        if self.viewer is None:
            pygame.init()
            self.viewer = pygame.display.set_mode((screen_width, screen_height))
            pygame.display.set_caption("Pendulum Environment")

        self.viewer.fill((255, 255, 255))

        world_center = (screen_width // 2, screen_height // 2)
        pole_length = self.l * 100
        theta = self.state[0]
        pendulum_x = world_center[0] + pole_length * np.sin(theta)
        pendulum_y = world_center[1] - pole_length * np.cos(theta)

        gfxdraw.line(self.viewer, world_center[0], world_center[1], int(pendulum_x), int(pendulum_y), (0, 0, 0))
        gfxdraw.filled_circle(self.viewer, int(pendulum_x), int(pendulum_y), 10, (0, 0, 255))
        pygame.display.flip()

    def _render_rgb_array(self):
        return np.zeros((500, 500, 3), dtype=np.uint8)  # Placeholder

    def close(self):
        if self.viewer is not None:
            import pygame
            pygame.quit()
            self.viewer = None