# -*- coding: utf-8 -*-
"""testing_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ERIa2VKCcewrtos0vD0bMa5wOuXNqbcc
"""

# import gymnasium
# from gymnasium.envs.registration import register
# register(
#     id='LinearFirstOrder-v0',
#     entry_point='my_gym_environments.envs:LinearFirstOrderEnv',
#     # max_episode_steps=300,

# )
# register(
#     id='LinearSecondOrder-v0',
#     entry_point='my_gym_environments.envs:LinearSecondOrderEnv',
#     # max_episode_steps=300,

# )
# register(
#     id='MyPendulum-v0',
#     entry_point='my_gym_environments.envs:MyPendulumEnv',
#     # max_episode_steps=300,

# )

import numpy as np

class OUNoise:
    def __init__(self, mu, sigma=0.3, theta=.15, dt=1e-2, x0=None):
        self.theta = theta
        self.mu = mu
        self.sigma = sigma
        self.dt = dt
        self.x0 = x0
        self.reset()

    def __call__(self):
        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \
            self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)
        self.x_prev = x
        return x

    def reset(self):
        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)

    def __repr__(self):
        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)

import copy
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import random
import time
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# device = torch.device("cpu")
print(device)

class Actor(nn.Module):
    def __init__(self, state_dim, action_dim, max_action, action_scale, action_add):
        super(Actor, self).__init__()
        self.action_scale = torch.FloatTensor(action_scale).to(device)
        self.action_add = torch.FloatTensor(action_add).to(device)

        self.l1 = nn.Linear(state_dim, 16)
        self.l2 = nn.Linear(16, 32)
        self.l3 = nn.Linear(32, 16)
        self.l4 = nn.Linear(16, action_dim)
        self.max_action = max_action

    def forward(self, state):
        a = F.relu(self.l1(state))
        a = F.relu(self.l2(a))
        a = F.relu(self.l3(a))
        a = self.l4(a)
        # a = self.action_scale * (torch.tanh(a) + self.action_add)
        a = self.action_scale * torch.tanh(a) + self.action_add
        a = a*self.max_action

        return a

class Critic(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(Critic, self).__init__()

        # Q1 architecture
        self.l1 = nn.Linear(state_dim + action_dim, 16)
        self.l2 = nn.Linear(16, 32)
        self.l3 = nn.Linear(32, 16)
        self.l4 = nn.Linear(16, 1)

        # Q2 architecture
        self.l5 = nn.Linear(state_dim + action_dim, 16)
        self.l6 = nn.Linear(16, 32)
        self.l7 = nn.Linear(32, 16)
        self.l8 = nn.Linear(16, 1)

    def forward(self, state, action):
        sa = torch.cat([state, action], 1)
        # print(sa.shape)
        q1 = F.relu(self.l1(sa))
        q1 = F.relu(self.l2(q1))
        q1 = F.relu(self.l3(q1))
        q1 = self.l4(q1)

        q2 = F.relu(self.l5(sa))
        q2 = F.relu(self.l6(q2))
        q2 = F.relu(self.l7(q2))
        q2 = self.l8(q2)
        return q1, q2

    def Q1(self, state, action):
        sa = torch.cat([state, action], 1)

        q1 = F.relu(self.l1(sa))
        q1 = F.relu(self.l2(q1))
        q1 = F.relu(self.l3(q1))
        q1 = self.l4(q1)
        return q1

class TD3(object):
    def __init__(
        self,
        state_dim,
        action_dim,
        max_action,
        action_scale, action_add,
        discount=0.99,
        tau=0.001,
        policy_noise=0.0,
        noise_clip=0.1,
        policy_freq=1
    ):
        self.actor = Actor(state_dim, action_dim, max_action, action_scale, action_add).to(device)
        self.actor_target = copy.deepcopy(self.actor)
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=1e-4)

        self.critic = Critic(state_dim, action_dim).to(device)
        self.critic_target = copy.deepcopy(self.critic)
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=1e-3)

        self.max_action = max_action
        self.discount = discount
        self.tau = tau
        self.policy_noise = policy_noise
        self.noise_clip = noise_clip
        self.policy_freq = policy_freq
        self.total_it = 0

    def select_action(self, state):
        state = torch.FloatTensor(state).to(device)
        action = self.actor(state).cpu().data.numpy()
        return action

    def train(self, replay_buffer, batch_size=256):
        self.total_it += 1
        # Sample replay buffer
        # tic = time.time()
        batch = random.sample(replay_buffer, batch_size)
        state, action, reward, next_state, not_done = zip(*batch)
        state = torch.FloatTensor(state).to(device)
        action = torch.FloatTensor(action).to(device)
        reward = torch.FloatTensor(reward).to(device)
        not_done = torch.FloatTensor(not_done).to(device)
        not_done = not_done.reshape([not_done.shape[0],1])
        next_state = torch.FloatTensor(next_state).to(device)
        with torch.no_grad():
            # Select action according to policy and add clipped noise
            noise = (
                torch.randn_like(action) * self.policy_noise
            ).clamp(-self.noise_clip, self.noise_clip)

            next_action = (
                self.actor_target(next_state) + noise
            ).clamp(-self.max_action, self.max_action)

            # Compute the target Q value
            target_Q1, target_Q2 = self.critic_target(next_state, next_action)
            target_Q = torch.min(target_Q1, target_Q2)
            target_Q = reward.reshape(target_Q.shape) + not_done * self.discount * target_Q

        # Get current Q estimates
        current_Q1, current_Q2 = self.critic(state, action)

        # Compute critic loss
        critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)

        # Optimize the critic
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()

        # Delayed policy updates
        if self.total_it % self.policy_freq == 0:

            # Compute actor losse
            actions = self.actor(state)
            actor_loss = -self.critic.Q1(state, actions).mean()

            # Optimize the actor
            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            self.actor_optimizer.step()

            # Update the frozen target models
            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):
                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)

            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):
                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)

            return actor_loss.item(), critic_loss.item()/len(batch)
        return 0, critic_loss.item()/len(batch)

    def save(self, filename):
        torch.save(self.critic.state_dict(), filename + "_critic.pt")
        torch.save(self.actor.state_dict(), filename + "_actor.pt")

    def load(self, filename):
        self.critic.load_state_dict(torch.load(filename + "_critic.pt", map_location=torch.device('cpu')))
        self.critic_target = copy.deepcopy(self.critic)

        self.actor.load_state_dict(torch.load(filename + "_actor.pt", map_location=torch.device('cpu')))
        self.actor_target = copy.deepcopy(self.actor)

import numpy as np
import torch
from collections import deque
from copy import deepcopy



device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class Agent():
    def __init__(self, state_dim, action_dim, max_action = 1, action_scale = [1.0], action_add=[0.0], batch_size = 64) -> None:
        self.TD = TD3(state_dim, action_dim, max_action, action_scale, action_add)
        self.steps_done = 0
        self.memory = deque(maxlen=100000)
        self.batch_size = batch_size

    def memorize(self, state, action, reward, next_state, not_done):
        self.memory.append([state, action, reward, next_state, not_done])

    def learn(self):
        if len(self.memory) < self.batch_size:
            return 0, 0
        err_actor, err_critic = self.TD.train(self.memory, self.batch_size)
        return err_actor, err_critic

    def act(self, state):
        action = self.TD.select_action(state)
        return action

from os import path
from typing import Optional
import numpy as np
import scipy as sp
import gym
from gym import spaces
# import gym
# from gym import spaces
from gym.envs.classic_control import utils
from gym.error import DependencyNotInstalled

class LinearFirstOrderEnv(gym.Env):

    def __init__(self,K,lam,lam2,w1,w2):

        # linear first order
        self.A = -2.0
        self.B = 3.0
        self.C = 1.0
        self.D = 0.0
        self.K = K #4
        self.lam = lam #0.8
        self.lam2 = lam2 #0.5
        self.w1 = w1 #1.0
        self.w2 = w2 #0.001

        self.dt = 0.05
        self.action_add = [0.0]
        self.action_scale = [1.0]
        self.min_action = -1.0
        self.max_action = 1.0 #step input
        self.min_state = -10.0
        self.max_state = 10.0
        self.min_K  = 0.0
        self.max_K  = 1000.0
        self.min_e = self.min_state - self.max_state
        self.max_e = self.max_state - self.min_state
        self.min_edot = (self.min_e - self.max_e)/self.dt
        self.max_edot = (self.max_e - self.min_e)/self.dt

        self.ref_model_min = self.min_edot + self.K*self.min_e # K >= 0
        self.ref_model_max = self.max_edot + self.K*self.max_e

        #obs =  [state, targ_state, model, targ_model, K]
        self.min_obs = np.array([self.min_state, self.min_state, self.ref_model_min, self.ref_model_min, self.min_K])
        self.max_obs = np.array([self.max_state, self.max_state, self.ref_model_max, self.ref_model_max, self.max_K])

        self.targ_state = 0.0 #desired step output
        self.init_state = 1.0
        self.test_targ_state = 0.0
        self.test_init_state = 1.0
        self.targ_m = 0.0

        self.observation_space = spaces.Box(low=self.min_obs, high=self.max_obs, shape=(5,), dtype=np.float32)
        self.action_space = spaces.Box(low=self.min_action, high=self.max_action, shape=(1,), dtype=np.float32)

    def step(self, u):
        # e = y_r - y (y=x, y_r=1), step output
        # e_dot = y_r_dot - y_dot
        u = u[0]
        done = False

        x = self.state
        dx = np.dot(self.A,x) + np.dot(self.B,u)
        new_x = x + self.dt*(dx)
        self.e = self.targ_state - x
        e_new = self.targ_state - new_x
        self.e_dot = (e_new - self.e)/self.dt
        self.m = self.e_dot + self.K * self.e
        self.ie += self.e*self.dt

        # e_dot = 0 - (self.A*x + self.B*u)
        # e = np.linalg.norm(e)
        # e_dot = np.linalg.norm(e_dot)
        # e = float(e)
        # e_dot = float(e_dot)
        # rewards = - (self.K**2 * abs(self.e)) - (self.K * abs(self.e_dot)) - 0.001*u**2 - 5*(abs(self.e * self.e_dot))
        # rewards = - (49 * abs(self.e)**2) - (7 * abs(self.e_dot)**2) - 0.5*(abs(self.e_dot + self.K * self.e)**2) - 0.001*u**2
        # IE_term  = self.IE + (abs(self.e))
                # rewards = abs(e_dot + 5*e)**2 + self.K*e_dot**2
        # rewards = -np.exp(-5*(self.dt*(self.i+1)) + 1000)
        # rewards = - (self.alpha **2) * (self.e_dot + self.K * self.e) **2  # -  self.alpha * (self.e)**2
        # rewards = self.alpha*(-(self.K**2 * abs(self.e)**2) - (self.alpha) * abs(self.e_dot)**2 - (2 * self.K/4)*(abs(self.e * self.e_dot)) - self.K * (self.ref_model)) - 0.001*u**2

        # rewards = - self.alpha * (self.K**2 * abs(self.e)**2) - (self.beta) * abs(self.e_dot)**2 - (2 * self.gamma * self.K * abs(self.e * self.e_dot)) - self.omega * (self.ref_model) - self.tau * u**2
        # rewards =  - self.K**2*abs(self.e)**2 - (0.5)*abs(self.e_dot)**2 - 2*self.K*abs(self.e * self.e_dot) - 0.001*u**2
        # rewards = - 20*self.K**2*abs(self.e)**2 - 5*(self.K/20)*abs(self.e_dot)**2 - 4*2*(self.K/10)*abs(self.e * self.e_dot) - 0.001*u**2

        # rewards = - (self.K**2 * self.e**2) - self.e_dot**2 - 0.001*u**2 - 2*abs(self.K * self.e * self.e_dot)

        # rewards = -25*abs(0.9*self.e_dot + 9*self.K * self.e)**2

        if abs(self.e) <= 0.01:
            done = True

        # if self.state <= (98/100)*self.targ_state:
            # rewards = -abs(self.m - self.targ_m)
        # else:
            # rewards = -abs(self.targ_state - self.state)
            # rewards = -(abs(self.m - self.targ_m) + 0.01*abs(self.targ_state - self.state))

        rewards = -(self.w1*abs(self.m - self.targ_m) + self.lam*abs(e_new)**2 + self.lam2*abs(self.e_dot)**2 + self.w2*u**2)

        self.state = new_x
        obs = self.get_obs()
        info = {'e': self.e, 'e_dot': self.e_dot, 'ref_model': self.m}
        return obs, rewards, done, False, info

    def reset(self, init_state = None, targ_state = None, K = None):

        if init_state is None:
            self.state = self.np_random.uniform(self.min_state, self.max_state)
        else:
            self.state = init_state

        if targ_state is None:
            # self.targ_state = self.np_random.uniform(self.min_state, self.max_state)
            self.targ_state = self.targ_state
        else:
            self.targ_state = targ_state

        # if K is not None:
            # self.K = K
            # self.K = self.np_random.uniform(self.min_K, self.max_K)
        # else:
            # self.K = self.K

        # self.state = float(self.state)
        # self.targ_state = float(self.targ_state)
        self.e = self.targ_state - self.state
        # self.e_dot = 0 - (self.A*self.state + self.B*0.0)
        self.e_dot = (self.e - self.e)/self.dt
        self.m = self.e_dot + self.K * self.e
        self.ie = self.e
        obs = self.get_obs()
        info = {'init_state':self.state, 'targ_state':self.targ_state, 'e': self.e, 'e_dot': self.e_dot, 'K': self.K, 'model': self.m}
        return obs, info

    def get_obs(self):
        # self.obs = np.array([self.state, ref_model, self.K])
        # self.obs = np.array([self.state, self.targ_state, self.e, self.e_dot, self.K])
        # self.obs = np.array([self.state])
        self.obs = np.array([self.state, self.targ_state, self.m, self.targ_m, self.K])
        return self.obs

!mkdir -p checkpoints/checkpoints_linearfirstorder
!pip install wandb
import gym
import numpy as np
import matplotlib.pyplot as plt
import wandb
import os

# Parameter ranges and means
K_range = np.linspace(0.01, 4, 25)
lam1_range = np.linspace(0.01, 2, 25)
lam2_range = np.linspace(0.01, 2, 25)
w1_range = np.linspace(0.01, 1, 25)
w2 = 0.001  # constant

K_mean = (4 + 0.01) / 2
lam1_mean = (2 + 0.01) / 2
lam2_mean = (2 + 0.01) / 2
w1_mean = (1 + 0.01) / 2
for lam2 in lam2_range[21:25]:

    env = LinearFirstOrderEnv(K=K_mean, lam=lam1_mean, lam2=lam2, w1=w1_mean, w2=0.001)

    class SimpleRunner:
        def __init__(self, num_episodes=150, eval_every=40, learn_every=1, noise_variance=0.1, viz=False, wandb_on=True, with_dth=False) -> None:
            self.env = env

            self.num_episodes = num_episodes
            self.max_action = float(self.env.max_action)
            self.K = self.env.K
            self.agent = Agent(self.env.observation_space.shape[0], self.env.action_space.shape[0], max_action=self.max_action, action_scale = self.env.action_scale, action_add = self.env.action_add)

            self.learn_every = learn_every
            self.noise_variance = noise_variance
            self.viz = viz
            self.eval_every = eval_every
            self.wandb_on = wandb_on
            self.avg_window = self.num_episodes//20

            self.init_state = self.env.init_state
            self.targ_state = self.env.targ_state
            self.test_init_state = self.env.test_init_state
            self.test_targ_state = self.env.test_targ_state
            self.new_reward_flag = 1
            self.with_dth = with_dth
            self.lam = self.env.lam
            self.lam2 = self.env.lam2
            self.w1 = self.env.w1
            self.w2 = self.env.w2

            if self.wandb_on:
                wandb.config = {
                    "num_episodes": num_episodes,
                    "learn_every": learn_every,
                    "noise_variance": noise_variance,
                    "eval_every": eval_every,
                    "K": self.K,
                    "max_action": self.max_action,
                    "lambda":self.lam,
                    "lambda2":self.lam2,
                    "w1":self.w1,
                    "w2":self.w2
                }
                wandb.init(project="TD3-errorcontrol-firstorderlinear", entity="rlmpc", config = wandb.config)

        def run(self):
            # print(gymnasium.pprint_registry())
            steps=0
            # print('K*******', self.K)

            # actor_noise = OUNoise(mu=np.zeros(self.env.action_space.shape[0]))
            score_history = [] # score is the sum of rewards of an episode
            best_score = self.env.reward_range[0]
            for episode in range(self.num_episodes):
                observation, info = self.env.reset(init_state=self.init_state, targ_state=self.targ_state)
                # observation, info = self.env.reset()
                # observation, info = self.env.reset(targ_state=self.targ_state)

                # print('info_after_ep_reset', info)

                done = False
                done_ = False
                score = 0
                episode_len = 0
                episode_action = 0
                episode_error = 0.0
                err_actor_episode = 0
                err_critic_episode = 0
                # score_rewards = []
                # while not (done or done_):
                for i in range(int(1000)):
                    actor_noise = np.random.normal(0, self.noise_variance, self.env.action_space.shape)
                    action = self.agent.act(observation) + actor_noise
                    # print('action', action)
                    observation_, reward, done, done_, vars = self.env.step(action)
                    # print(observation, observation_)

                    sys_error = vars['e'] #old state and e calculated wrt the old state
                    sys_error_dot = vars['e_dot'] #old state and e calculated wrt the old state
                    ref_model = vars['ref_model']

                    self.agent.memorize(observation, action, reward, observation_, not (done or done_))
                    observation = observation_

                    if steps % self.learn_every == 0:
                         err_actor, err_critic = self.agent.learn()
                    steps = (steps + 1) % self.learn_every
                    score += reward
                    episode_len += 1
                    episode_action += action
                    episode_error += abs(sys_error)
                    # score_rewards.append(reward)
                    err_actor_episode += err_actor
                    err_critic_episode += err_critic
                    # print('observation, step', observation, episode_len)
                    # print('sys_error, sys_error_dot, action, reward', sys_error, sys_error_dot, action, reward)
                    # print('\n')
                    if self.wandb_on:
                        # wandb.log({'sys_state': sys_state, 'action': action, "step": i, 'reward': reward, 'sys_error': sys_error})
                        wandb.log({'sys_state': observation[0], 'action': action, "step": episode_len, 'reward': reward, 'sys_error': sys_error, 'sys_error_dot': sys_error_dot, 'model': ref_model})
                    # if (done or done_):
                        # print("episode done------------------------------")
                        # break

                score_history.append(score)
                avg_score = np.mean(score_history[-self.avg_window:])
                if avg_score > best_score:
                    best_score = avg_score
                    # best_score_rewards = score_rewards
                    self.best_model_episode = episode

                    if self.new_reward_flag:
                        self.agent.TD.save(f"best_model_"+ str(self.K) + "_targ" + str(int(np.rad2deg(self.targ_state))))
                    else:
                        self.agent.TD.save(f"best_model_"+ "targ" + str(int(np.rad2deg(self.targ_state))))

                train_avg_reward_of_episode = score/episode_len
                train_avg_action_of_episode = episode_action/episode_len
                train_avg_error_of_episode = episode_error/episode_len

                print(f"{episode}, {episode_len}, avg_reward:{train_avg_reward_of_episode}, final_obs_ep:{observation}, final_e_ep:{sys_error}")
                # print("episode done------------------------------")

                if self.wandb_on:
                    wandb.log({"train_episode":episode, "train_score":score, "train_episode_len":episode_len, "train_episode_actor_loss": err_actor_episode, "train_episode_critic_loss":err_critic_episode, "train_avg_reward_of_episode": train_avg_reward_of_episode, "train_avg_action_of_episode": train_avg_action_of_episode, "train_avg_error_of_episode": train_avg_error_of_episode})

                if episode % self.learn_every == 0:
                    # self.agent.TD.save("checkpoints_linearfirstorder" + str(int(self.with_dth)) + "/" + str(episode))
                    self.agent.TD.save("checkpoints/checkpoints_linearfirstorder" + "/" + str(episode))

                if (episode % self.eval_every == 0) and (episode != 0):
                    self.eval(episode)

                # if abs(sys_state - self.targ_state) <= 0.001:
                    # break

            if self.wandb_on:
                wandb.log({"train_best_score": best_score, "train_best_model_episode": self.best_model_episode})

        def eval(self, episode):
            observation, _ = self.env.reset(init_state=self.test_init_state, targ_state=self.test_targ_state)
            done = False
            done_ = False
            score = 0
            episode_error = 0.0
            step = 0

            # while not (done or done_):
            for i in range(int(1000)):
                action = self.agent.act(observation)
                observation_, reward, done, done_, vars = self.env.step(action)
                sys_error = vars['e']
                sys_error_dot = vars['e_dot']
                ref_model = vars['ref_model']

                observation = observation_
                score += reward
                episode_error += sys_error
                step+=1

                if self.wandb_on:
                    wandb.log({"eval_theta_ep_" + str(episode): observation[0], "eval_action_ep_" + str(episode): action, "eval_step" :step, "eval_reward_ep_" + str(episode): reward, "eval_error_ep_" + str(episode): sys_error, "eval_edot_ep_" + str(episode): sys_error_dot, "eval_errormodel_ep_" + str(episode): ref_model})
            print('--------------------------')
            print(f"eval_episode: {episode}, eval_score: {score}")
            print(f"eval_avg_reward: {score/step}, eval_episode_len: {step}, eval_final_obs: {observation}, eval_final_sys_error: {sys_error}, eval_e_dot: {sys_error_dot}")
            print('---------------------------')

            # if self.wandb_on:
                # wandb.log({"eval_episode": episode, "eval_episode_len": step, "eval_score_ep_" + str(episode): score, "eval_episode_error_ep_" + str(episode): episode_error, "eval_avg_reward": score/step})

        def test_after_training(self):
            self.env = env
            self.agent = Agent(self.env.observation_space.shape[0], self.env.action_space.shape[0], max_action=self.max_action)

            if self.new_reward_flag:
                self.agent.TD.load(f"best_model_" + str(self.K) + "_targ" + str(int(np.rad2deg(self.targ_state))))

            observation, _ = self.env.reset(targ_state=self.test_targ_state, init_state=self.test_init_state)
            done = False
            score = 0
            test_step = 0
            episode_error = 0.0
            step_arr = []
            sys_error_arr = []
            test_sys_state = []
            # Run the test episode
            for i in range(2000):
                action = self.agent.act(observation)
                observation_, reward, done, _, vars = self.env.step(action)
                sys_error = vars['e']
                sys_error_dot = vars['e_dot']
                ref_model = vars['ref_model']

                observation = observation_
                score += reward
                test_step += 1
                episode_error += sys_error
                step_arr.append(test_step)
                sys_error_arr.append(sys_error)
                test_sys_state.append(observation[0])
                if self.wandb_on and i > 0:
                    wandb.log({
                        "test_reward": reward,
                        "test_sys_state": observation[0],
                        "test_action": action,
                        "test_step": test_step,
                        "test_sys_error": sys_error,
                        "test_sys_error_dot": sys_error_dot,
                        "test_sys_refmodel": ref_model
                    })
                if self.wandb_on and i == 0:
                    wandb.log({
                        "test_sys_state": self.test_init_state,
                        "test_step": test_step
                    })
            print("this is the test_sys_state",test_sys_state)
            # Calculate performance metrics
            rise_time = self.calculate_rise_time(test_sys_state, step_arr, test_sys_state[-1])
            sse = self.calculate_sse(test_sys_state[-1], self.test_targ_state)

            print('\n')
            print(f"test_avg_reward: {score/test_step}, test_episode_len: {test_step}, test_final_obs: {observation}, test_final_sys_error: {sys_error}")
            print(f"Rise Time: {rise_time}")
            print(f"Steady-State Error (Percentage): {sse}%")
            print('\n')
            self.test_rise_time = rise_time
            self.test_sse_percentage = sse
            if self.wandb_on:
              wandb.log({
                "test_score": score,
                "test_episode_len": test_step,
                "test_error_total": episode_error,
                "test_avg_reward": score/test_step,
                "test_rise_time": rise_time,
                "test_sse_percentage": sse,
                "K": self.K,                   # Log K value
                "lam1": self.lam,               # Log lam1 value
                "lam2": self.lam2,              # Log lam2 value
                "w1": self.w1,                  # Log w1 value
                "w2": self.w2                   # Log w2 value
              })

       # Plot system error over time
            plt.plot(step_arr, sys_error_arr)
            plt.xlabel('Time Step')
            plt.ylabel('System Error')
            plt.title('System Error Over Time')
            print(f"test system state: {observation[0]}")

        def calculate_rise_time(self, test_states, time_steps, final_value):
            required_value = 0.9 * final_value
            closest_value = test_states[0]
            closest_time = time_steps[0]
            initial_difference = abs(closest_value - required_value)

            for i in range(len(test_states)):
                difference = abs(required_value - test_states[i])
                if difference < initial_difference:
                    closest_value = test_states[i]
                    closest_time = time_steps[i]
                    initial_difference = difference

            rise_time = (closest_time - time_steps[0]) * 0.05
            return rise_time

        # def calculate_rise_time(self, test_states, time_steps, final_value):
        #     closest_value = test_states[0]
        #     closest_time = time_steps[0]

        #     for i in range(0, len(test_states)):
        #         difference = abs(0.9*final_value - test_states[i])
        #         if difference <= closest_value:
        #             closest_value = test_states[i]
        #             closest_time = time_steps[i]

        #     rise_time = (closest_time - time_steps[0])*0.05
        #     return rise_time

        def calculate_sse(self, final_value, target_value):
            sse = abs(target_value - final_value)
            sse_percentage = (sse / 1) * 100
            return sse_percentage

    if __name__ == "__main__":
        runner = SimpleRunner(viz = False)
        runner.run()
        runner.test_after_training()
        print(f"Stored Rise Time: {runner.test_rise_time}")
        print(f"Stored Steady-State Error Percentage: {runner.test_sse_percentage}%")

!mkdir -p checkpoints/checkpoints_linearfirstorder
!pip install wandb
import gym
import numpy as np
import matplotlib.pyplot as plt
import wandb
import os

# Parameter ranges and means
K_range = np.linspace(0.01, 4, 25)
lam1_range = np.linspace(0.01, 2, 25)
lam2_range = np.linspace(0.01, 2, 25)
w1_range = np.linspace(0.01, 1, 25)
w2 = 0.001  # constant

K_mean = (4 + 0.01) / 2
lam1_mean = (2 + 0.01) / 2
lam2_mean = (2 + 0.01) / 2
w1_mean = (1 + 0.01) / 2
for w1 in w1_range:

    env = LinearFirstOrderEnv(K=K_mean, lam=lam1_mean, lam2=lam2_mean, w1=w1, w2=0.001)

    class SimpleRunner:
        def __init__(self, num_episodes=150, eval_every=40, learn_every=1, noise_variance=0.1, viz=False, wandb_on=True, with_dth=False) -> None:
            self.env = env

            self.num_episodes = num_episodes
            self.max_action = float(self.env.max_action)
            self.K = self.env.K
            self.agent = Agent(self.env.observation_space.shape[0], self.env.action_space.shape[0], max_action=self.max_action, action_scale = self.env.action_scale, action_add = self.env.action_add)

            self.learn_every = learn_every
            self.noise_variance = noise_variance
            self.viz = viz
            self.eval_every = eval_every
            self.wandb_on = wandb_on
            self.avg_window = self.num_episodes//20

            self.init_state = self.env.init_state
            self.targ_state = self.env.targ_state
            self.test_init_state = self.env.test_init_state
            self.test_targ_state = self.env.test_targ_state
            self.new_reward_flag = 1
            self.with_dth = with_dth
            self.lam = self.env.lam
            self.lam2 = self.env.lam2
            self.w1 = self.env.w1
            self.w2 = self.env.w2

            if self.wandb_on:
                wandb.config = {
                    "num_episodes": num_episodes,
                    "learn_every": learn_every,
                    "noise_variance": noise_variance,
                    "eval_every": eval_every,
                    "K": self.K,
                    "max_action": self.max_action,
                    "lambda":self.lam,
                    "lambda2":self.lam2,
                    "w1":self.w1,
                    "w2":self.w2
                }
                wandb.init(project="TD3-errorcontrol-firstorderlinear", entity="rlmpc", config = wandb.config)

        def run(self):
            # print(gymnasium.pprint_registry())
            steps=0
            # print('K*******', self.K)

            # actor_noise = OUNoise(mu=np.zeros(self.env.action_space.shape[0]))
            score_history = [] # score is the sum of rewards of an episode
            best_score = self.env.reward_range[0]
            for episode in range(self.num_episodes):
                observation, info = self.env.reset(init_state=self.init_state, targ_state=self.targ_state)
                # observation, info = self.env.reset()
                # observation, info = self.env.reset(targ_state=self.targ_state)

                # print('info_after_ep_reset', info)

                done = False
                done_ = False
                score = 0
                episode_len = 0
                episode_action = 0
                episode_error = 0.0
                err_actor_episode = 0
                err_critic_episode = 0
                # score_rewards = []
                # while not (done or done_):
                for i in range(int(1000)):
                    actor_noise = np.random.normal(0, self.noise_variance, self.env.action_space.shape)
                    action = self.agent.act(observation) + actor_noise
                    # print('action', action)
                    observation_, reward, done, done_, vars = self.env.step(action)
                    # print(observation, observation_)

                    sys_error = vars['e'] #old state and e calculated wrt the old state
                    sys_error_dot = vars['e_dot'] #old state and e calculated wrt the old state
                    ref_model = vars['ref_model']

                    self.agent.memorize(observation, action, reward, observation_, not (done or done_))
                    observation = observation_

                    if steps % self.learn_every == 0:
                         err_actor, err_critic = self.agent.learn()
                    steps = (steps + 1) % self.learn_every
                    score += reward
                    episode_len += 1
                    episode_action += action
                    episode_error += abs(sys_error)
                    # score_rewards.append(reward)
                    err_actor_episode += err_actor
                    err_critic_episode += err_critic
                    # print('observation, step', observation, episode_len)
                    # print('sys_error, sys_error_dot, action, reward', sys_error, sys_error_dot, action, reward)
                    # print('\n')
                    if self.wandb_on:
                        # wandb.log({'sys_state': sys_state, 'action': action, "step": i, 'reward': reward, 'sys_error': sys_error})
                        wandb.log({'sys_state': observation[0], 'action': action, "step": episode_len, 'reward': reward, 'sys_error': sys_error, 'sys_error_dot': sys_error_dot, 'model': ref_model})
                    # if (done or done_):
                        # print("episode done------------------------------")
                        # break

                score_history.append(score)
                avg_score = np.mean(score_history[-self.avg_window:])
                if avg_score > best_score:
                    best_score = avg_score
                    # best_score_rewards = score_rewards
                    self.best_model_episode = episode

                    if self.new_reward_flag:
                        self.agent.TD.save(f"best_model_"+ str(self.K) + "_targ" + str(int(np.rad2deg(self.targ_state))))
                    else:
                        self.agent.TD.save(f"best_model_"+ "targ" + str(int(np.rad2deg(self.targ_state))))

                train_avg_reward_of_episode = score/episode_len
                train_avg_action_of_episode = episode_action/episode_len
                train_avg_error_of_episode = episode_error/episode_len

                print(f"{episode}, {episode_len}, avg_reward:{train_avg_reward_of_episode}, final_obs_ep:{observation}, final_e_ep:{sys_error}")
                # print("episode done------------------------------")

                if self.wandb_on:
                    wandb.log({"train_episode":episode, "train_score":score, "train_episode_len":episode_len, "train_episode_actor_loss": err_actor_episode, "train_episode_critic_loss":err_critic_episode, "train_avg_reward_of_episode": train_avg_reward_of_episode, "train_avg_action_of_episode": train_avg_action_of_episode, "train_avg_error_of_episode": train_avg_error_of_episode})

                if episode % self.learn_every == 0:
                    # self.agent.TD.save("checkpoints_linearfirstorder" + str(int(self.with_dth)) + "/" + str(episode))
                    self.agent.TD.save("checkpoints/checkpoints_linearfirstorder" + "/" + str(episode))

                if (episode % self.eval_every == 0) and (episode != 0):
                    self.eval(episode)

                # if abs(sys_state - self.targ_state) <= 0.001:
                    # break

            if self.wandb_on:
                wandb.log({"train_best_score": best_score, "train_best_model_episode": self.best_model_episode})

        def eval(self, episode):
            observation, _ = self.env.reset(init_state=self.test_init_state, targ_state=self.test_targ_state)
            done = False
            done_ = False
            score = 0
            episode_error = 0.0
            step = 0

            # while not (done or done_):
            for i in range(int(1000)):
                action = self.agent.act(observation)
                observation_, reward, done, done_, vars = self.env.step(action)
                sys_error = vars['e']
                sys_error_dot = vars['e_dot']
                ref_model = vars['ref_model']

                observation = observation_
                score += reward
                episode_error += sys_error
                step+=1

                if self.wandb_on:
                    wandb.log({"eval_theta_ep_" + str(episode): observation[0], "eval_action_ep_" + str(episode): action, "eval_step" :step, "eval_reward_ep_" + str(episode): reward, "eval_error_ep_" + str(episode): sys_error, "eval_edot_ep_" + str(episode): sys_error_dot, "eval_errormodel_ep_" + str(episode): ref_model})
            print('--------------------------')
            print(f"eval_episode: {episode}, eval_score: {score}")
            print(f"eval_avg_reward: {score/step}, eval_episode_len: {step}, eval_final_obs: {observation}, eval_final_sys_error: {sys_error}, eval_e_dot: {sys_error_dot}")
            print('---------------------------')

            # if self.wandb_on:
                # wandb.log({"eval_episode": episode, "eval_episode_len": step, "eval_score_ep_" + str(episode): score, "eval_episode_error_ep_" + str(episode): episode_error, "eval_avg_reward": score/step})

        def test_after_training(self):
            self.env = env
            self.agent = Agent(self.env.observation_space.shape[0], self.env.action_space.shape[0], max_action=self.max_action)

            if self.new_reward_flag:
                self.agent.TD.load(f"best_model_" + str(self.K) + "_targ" + str(int(np.rad2deg(self.targ_state))))

            observation, _ = self.env.reset(targ_state=self.test_targ_state, init_state=self.test_init_state)
            done = False
            score = 0
            test_step = 0
            episode_error = 0.0
            step_arr = []
            sys_error_arr = []
            test_sys_state = []
            # Run the test episode
            for i in range(2000):
                action = self.agent.act(observation)
                observation_, reward, done, _, vars = self.env.step(action)
                sys_error = vars['e']
                sys_error_dot = vars['e_dot']
                ref_model = vars['ref_model']

                observation = observation_
                score += reward
                test_step += 1
                episode_error += sys_error
                step_arr.append(test_step)
                sys_error_arr.append(sys_error)
                test_sys_state.append(observation[0])
                if self.wandb_on and i > 0:
                    wandb.log({
                        "test_reward": reward,
                        "test_sys_state": observation[0],
                        "test_action": action,
                        "test_step": test_step,
                        "test_sys_error": sys_error,
                        "test_sys_error_dot": sys_error_dot,
                        "test_sys_refmodel": ref_model
                    })
                if self.wandb_on and i == 0:
                    wandb.log({
                        "test_sys_state": self.test_init_state,
                        "test_step": test_step
                    })
            print("this is the test_sys_state",test_sys_state)
            # Calculate performance metrics
            rise_time = self.calculate_rise_time(test_sys_state, step_arr, test_sys_state[-1])
            sse = self.calculate_sse(test_sys_state[-1], self.test_targ_state)

            print('\n')
            print(f"test_avg_reward: {score/test_step}, test_episode_len: {test_step}, test_final_obs: {observation}, test_final_sys_error: {sys_error}")
            print(f"Rise Time: {rise_time}")
            print(f"Steady-State Error (Percentage): {sse}%")
            print('\n')
            self.test_rise_time = rise_time
            self.test_sse_percentage = sse
            if self.wandb_on:
              wandb.log({
                "test_score": score,
                "test_episode_len": test_step,
                "test_error_total": episode_error,
                "test_avg_reward": score/test_step,
                "test_rise_time": rise_time,
                "test_sse_percentage": sse,
                "K": self.K,                   # Log K value
                "lam1": self.lam,               # Log lam1 value
                "lam2": self.lam2,              # Log lam2 value
                "w1": self.w1,                  # Log w1 value
                "w2": self.w2                   # Log w2 value
              })

       # Plot system error over time
            plt.plot(step_arr, sys_error_arr)
            plt.xlabel('Time Step')
            plt.ylabel('System Error')
            plt.title('System Error Over Time')
            print(f"test system state: {observation[0]}")

        def calculate_rise_time(self, test_states, time_steps, final_value):
            required_value = 0.9 * final_value
            closest_value = test_states[0]
            closest_time = time_steps[0]
            initial_difference = abs(closest_value - required_value)

            for i in range(len(test_states)):
                difference = abs(required_value - test_states[i])
                if difference < initial_difference:
                    closest_value = test_states[i]
                    closest_time = time_steps[i]
                    initial_difference = difference

            rise_time = (closest_time - time_steps[0]) * 0.05
            return rise_time

        # def calculate_rise_time(self, test_states, time_steps, final_value):
        #     closest_value = test_states[0]
        #     closest_time = time_steps[0]

        #     for i in range(0, len(test_states)):
        #         difference = abs(0.9*final_value - test_states[i])
        #         if difference <= closest_value:
        #             closest_value = test_states[i]
        #             closest_time = time_steps[i]

        #     rise_time = (closest_time - time_steps[0])*0.05
        #     return rise_time

        def calculate_sse(self, final_value, target_value):
            sse = abs(target_value - final_value)
            sse_percentage = (sse / 1) * 100
            return sse_percentage

    if __name__ == "__main__":
        runner = SimpleRunner(viz = False)
        runner.run()
        runner.test_after_training()
        print(f"Stored Rise Time: {runner.test_rise_time}")
        print(f"Stored Steady-State Error Percentage: {runner.test_sse_percentage}%")